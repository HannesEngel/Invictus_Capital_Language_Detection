{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 0\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Ship shape and Bristol fashion</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Know the ropes</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Graveyard shift</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Milk of human kindness</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Touch with a barge-pole - Wouldn't</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 text language\n",
       "0      Ship shape and Bristol fashion  English\n",
       "1                      Know the ropes  English\n",
       "2                     Graveyard shift  English\n",
       "3              Milk of human kindness  English\n",
       "4  Touch with a barge-pole - Wouldn't  English"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing and inspecting the data\n",
    "df = pd.read_csv('lang_data.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>2761</td>\n",
       "      <td>2839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>unique</td>\n",
       "      <td>2752</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>top</td>\n",
       "      <td>As different as chalk and cheese</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>freq</td>\n",
       "      <td>2</td>\n",
       "      <td>2077</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    text language\n",
       "count                               2761     2839\n",
       "unique                              2752        3\n",
       "top     As different as chalk and cheese  English\n",
       "freq                                   2     2077"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataframe has 2839 rows.\n"
     ]
    }
   ],
   "source": [
    "# inspecting the data\n",
    "initial_rows = df.shape[0]\n",
    "print(f\"The dataframe has {initial_rows} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['English' 'Afrikaans' 'Nederlands']\n"
     ]
    }
   ],
   "source": [
    "# checking whether we have any inconsitencies in the labels - fortunately, we only have 3 unique labels assigned to eact string of text\n",
    "print(df['language'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "English       73.159563\n",
       "Afrikaans     23.635083\n",
       "Nederlands     3.205354\n",
       "Name: language, dtype: float64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking the distribution of the languages\n",
    "df['language'].value_counts(normalize=True) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above we see that we have an imbalanced dataset, with the vast majority of the observations belonging to either the English or Afrikaans languages. Roughly 3% of the text is Dutch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ship shape and Bristol fashion\n",
      "Know the ropes\n",
      "Graveyard shift\n",
      "Milk of human kindness\n",
      "Touch with a barge-pole - Wouldn't\n",
      "Sy kan altyd my battery natpiepie.\n",
      "When the shit hits the fan\n",
      "nan\n",
      "Egg on\n",
      "Drag race\n",
      "As queer as a nine bob note\n",
      "nan\n",
      "Run the gauntlet\n",
      "Raining cats and dogs\n",
      "No laughing matter\n",
      "Run of the mill\n",
      "Piet Pompies van Soetmelksvlei is nie van hier nie.\n",
      "nan\n",
      "A drop in the ocean\n",
      "In a trice\n",
      "Auld lang syne\n"
     ]
    }
   ],
   "source": [
    "# checking the nature of the text column in the dataframe\n",
    "# print the first 20 text entries\n",
    "for i in range(0,21):\n",
    "    print(df['text'][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above we can see that this column will need some engineering before being passed to the model. For example. remove hypens and convert all the words to lower case. This will be done in the Feature engineering section below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2839 rows in total, of which 78 contain missing values.\n",
      "Percentage missing data = 2.75\n"
     ]
    }
   ],
   "source": [
    "# checking for any missing data\n",
    "df.isnull().values.any() # this line of code returns True if any data is missing, else it returns False\n",
    "df_reduced = df.copy()\n",
    "df_reduced = df_reduced.dropna(how = \"any\")\n",
    "reduced_rows = df_reduced.shape[0]\n",
    "print(f\"There are {initial_rows} rows in total, of which {initial_rows - reduced_rows} contain missing values.\")\n",
    "\n",
    "percentage_missing_data = (initial_rows - reduced_rows)/initial_rows*100\n",
    "percentage_missing_data = \"{:.2f}\".format(percentage_missing_data)\n",
    "print(f\"Percentage missing data = {percentage_missing_data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The missing data represents a small proportion of the total dataframe. We will therefore omit the rows where there are missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "English       74.429555\n",
       "Afrikaans     23.143788\n",
       "Nederlands     2.426657\n",
       "Name: language, dtype: float64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking the distribution of the languages after having dropped some rows\n",
    "# this is to check which languages have more missing values \n",
    "df_reduced['language'].value_counts(normalize=True) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the above figures to those we observed before omitting the missing value rows, we see that most of the missing values were assigned to either the Afrikaans or Dutch languages. This exacerbates the imbalanced dataset problem we are faced with. This will be dealt with in the Modeling section below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def process_sentence(sentence):\n",
    "    '''Removes all special characters from the sentence. It will also strip out\n",
    "    extra whitespace and makes the string lowercase.\n",
    "    '''\n",
    "    return re.sub(r'[\\\\\\\\/:*«`\\'?¿\";!<>,.|-]', '', sentence.lower().strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# clean each of the sentences in the dataset\n",
    "X = df_reduced[\"text\"].apply(process_sentence)\n",
    "y = df_reduced[\"language\"]\n",
    "\n",
    "# split all our sentences\n",
    "elements = (' '.join([sentence for sentence in X])).split()\n",
    "\n",
    "# splitting the original set into a training set and a test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2208, 553)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train), len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Languages in our dataset: {'English', 'Afrikaans', 'Nederlands'}\n"
     ]
    }
   ],
   "source": [
    "languages = set(y)\n",
    "print(\"Languages in our dataset: {}\".format(languages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Shapes:\n",
      "\tTrain set: \t\t(2208,) \n",
      "\tTest set: \t\t(553,)\n",
      "Totals:\n",
      "\tWords in our Dataset: 15249\n",
      "\tLanguages: 3\n"
     ]
    }
   ],
   "source": [
    "print(\"Feature Shapes:\")\n",
    "print(\"\\tTrain set: \\t\\t{}\".format(X_train.shape),\n",
    "      \"\\n\\tTest set: \\t\\t{}\".format(X_test.shape))\n",
    "print(\"Totals:\\n\\tWords in our Dataset: {}\\n\\tLanguages: {}\".format(len(elements), len(languages)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1402                              keep a stiff upper lip\n",
       "2616    ontwyk die uitvraer want hy is ook ‘n verklapper\n",
       "2292            there is more than one way to kill a cat\n",
       "2108            fools rush in where angels fear to tread\n",
       "464                                       meat and drink\n",
       "                              ...                       \n",
       "2679                                      gloom and doom\n",
       "1248                               the toast of the town\n",
       "675                                       binge drinking\n",
       "1262                                         chick flick\n",
       "346                                               my bad\n",
       "Name: text, Length: 553, dtype: object"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "771     seks is soos jou salaristjek mens praat nie re...\n",
      "1028         sit die babatjie op iemand anders se drumpel\n",
      "2464                                       anchors aweigh\n",
      "1787    behandel jou meerdere met beleefdheid omdat di...\n",
      "1492                                           à la carte\n",
      "Name: text, dtype: object\n",
      "771     Afrikaans\n",
      "1028    Afrikaans\n",
      "2464      English\n",
      "1787    Afrikaans\n",
      "1492      English\n",
      "Name: language, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# lets look at our training data\n",
    "print(X_train[:5])\n",
    "print(y_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lookup_tables(text):\n",
    "    \"\"\"Create lookup tables for vocabulary\n",
    "    :param text: The text split into words\n",
    "    :return: A tuple of dicts (vocab_to_int, int_to_vocab)\n",
    "    \"\"\"\n",
    "    vocab = set(text)\n",
    "    \n",
    "    vocab_to_int = {word: i for i, word in enumerate(vocab)}\n",
    "    int_to_vocab = {v:k for k, v in vocab_to_int.items()}\n",
    "    \n",
    "    return vocab_to_int, int_to_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary of our dataset: 4444\n"
     ]
    }
   ],
   "source": [
    "elements.append(\"<UNK>\")\n",
    "\n",
    "# map our vocabulary to int\n",
    "vocab_to_int, int_to_vocab = create_lookup_tables(elements)\n",
    "languages_to_int, int_to_languages = create_lookup_tables(y)\n",
    "\n",
    "print(\"Vocabulary of our dataset: {}\".format(len(vocab_to_int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_int(data, data_int):\n",
    "    \"\"\"Converts all our text to integers\n",
    "    :param data: The text to be converted\n",
    "    :return: All sentences in ints\n",
    "    \"\"\"\n",
    "    all_items = []\n",
    "    for sentence in data: \n",
    "        all_items.append([data_int[word] if word in data_int else data_int[\"<UNK>\"] for word in sentence.split()])\n",
    "    \n",
    "    return all_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert our inputs to numbers\n",
    "X_test_encoded = convert_to_int(X_test, vocab_to_int)\n",
    "X_train_encoded = convert_to_int(X_train, vocab_to_int)\n",
    "\n",
    "y_data = convert_to_int(y_test, languages_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Applications/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Applications/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# encoding our labels using OneHotEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "enc = OneHotEncoder()\n",
    "\n",
    "enc.fit(y_data)\n",
    "\n",
    "# One hot encoding our outputs\n",
    "y_train_encoded = enc.fit_transform(convert_to_int(y_train, languages_to_int)).toarray()\n",
    "y_test_encoded = enc.fit_transform(convert_to_int(y_test, languages_to_int)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]] \n",
      " 771     Afrikaans\n",
      "1028    Afrikaans\n",
      "2464      English\n",
      "1787    Afrikaans\n",
      "1492      English\n",
      "2700      English\n",
      "639       English\n",
      "1169    Afrikaans\n",
      "1841      English\n",
      "2709      English\n",
      "Name: language, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# sample of our encoding\n",
    "print(y_train_encoded[:10],'\\n', y_train[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 2.0.0\n"
     ]
    }
   ],
   "source": [
    "from distutils.version import LooseVersion\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "\n",
    "# check TensorFlow version\n",
    "assert LooseVersion(tf.__version__) >= LooseVersion('1.0'), 'Please use TensorFlow version 1.0 or newer'\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import Keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train length max: 65\n",
      "Test length max: 35\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nFrom the below we see that the longest text (in the both the training and test text columns) contains 65 words\\n'"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking the max length of a text string\n",
    "train_lengths = []\n",
    "test_lengths = []\n",
    "\n",
    "for text in X_train_encoded:\n",
    "    train_lengths.append(len(text))\n",
    "print(f\"Train length max: {max(train_lengths)}\")\n",
    "\n",
    "for text in X_test_encoded:\n",
    "    test_lengths.append(len(text))\n",
    "print(f\"Test length max: {max(test_lengths)}\")\n",
    "\n",
    "'''\n",
    "From the below we see that the longest text (in the both the training and test text columns) contains 65 words\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "max_sentence_length = 200 # some room for having longer pieces of text\n",
    "embedding_vector_length = 300\n",
    "dropout = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# truncate and pad input sentences\n",
    "X_train_pad = sequence.pad_sequences(X_train_encoded, maxlen=max_sentence_length)\n",
    "X_test_pad = sequence.pad_sequences(X_test_encoded, maxlen=max_sentence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 200, 300)          1333200   \n",
      "_________________________________________________________________\n",
      "lstm_11 (LSTM)               (None, 200, 256)          570368    \n",
      "_________________________________________________________________\n",
      "lstm_12 (LSTM)               (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 3)                 771       \n",
      "=================================================================\n",
      "Total params: 2,429,651\n",
      "Trainable params: 2,429,651\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(len(vocab_to_int), embedding_vector_length, input_length=max_sentence_length))\n",
    "model.add(LSTM(256, return_sequences=True, dropout=dropout, recurrent_dropout=dropout))\n",
    "model.add(LSTM(256, dropout=dropout, recurrent_dropout=dropout))\n",
    "model.add(Dense(len(languages), activation='softmax'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "2208/2208 [==============================] - 52s 24ms/step - loss: 0.4776 - accuracy: 0.7929\n",
      "Epoch 2/5\n",
      "2208/2208 [==============================] - 42s 19ms/step - loss: 0.3389 - accuracy: 0.8501\n",
      "Epoch 3/5\n",
      "2208/2208 [==============================] - 42s 19ms/step - loss: 0.2472 - accuracy: 0.9177\n",
      "Epoch 4/5\n",
      "2208/2208 [==============================] - 42s 19ms/step - loss: 0.1408 - accuracy: 0.9586\n",
      "Epoch 5/5\n",
      "2208/2208 [==============================] - 42s 19ms/step - loss: 0.0879 - accuracy: 0.9725\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1a48ed7d50>"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(X_train_pad, y_train_encoded, epochs=5, batch_size=256, class_weight = 'auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentence(model, sentence):\n",
    "    \"\"\"Converts the text and sends it to the model for classification\n",
    "    :param sentence: The text to predict\n",
    "    :return: string - The language of the sentence\n",
    "    \"\"\"\n",
    "    \n",
    "    # Clean the sentence\n",
    "    sentence = process_sentence(sentence)\n",
    "    \n",
    "    # Transform and pad it before using the model to predict\n",
    "    x = np.array(convert_to_int([sentence], vocab_to_int))\n",
    "    x = sequence.pad_sequences(x, maxlen=max_sentence_length)\n",
    "    \n",
    "    prediction = model.predict(x)\n",
    "    \n",
    "    # Get the highest prediction\n",
    "    lang_index = np.argmax(prediction)\n",
    "    \n",
    "    return int_to_languages[lang_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the initial model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 97.77%\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation of the initial model\n",
    "scores = model.evaluate(X_test_pad, y_test_encoded, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions\n",
    "y_pred = []\n",
    "\n",
    "for text in X_test:\n",
    "    y_pred.append(predict_sentence(model, text)) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new dataframe with the actual and predicted languages\n",
    "\"\"\"\n",
    "This will help identify where the model is giving incorrect predictions.\n",
    "I suspect that the model will incorrectly allocate Dutch strings to Afrikaans predictions.\n",
    "\"\"\" \n",
    "def create_df(y_pred):\n",
    "    act_vs_pred = pd.DataFrame(data = [np.array(y_test), y_pred]).T\n",
    "    act_vs_pred.columns = ['Actual', 'Predicted']\n",
    "    return act_vs_pred\n",
    "\n",
    "# summarising the predictions across the actual language classes\n",
    "def summary(dataframe):\n",
    "    summary = pd.crosstab(dataframe.Actual, dataframe.Predicted)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>Afrikaans</th>\n",
       "      <th>English</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Afrikaans</td>\n",
       "      <td>123</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>English</td>\n",
       "      <td>0</td>\n",
       "      <td>411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Nederlands</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted   Afrikaans  English\n",
       "Actual                        \n",
       "Afrikaans         123        7\n",
       "English             0      411\n",
       "Nederlands          8        4"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act_vs_pred = create_df(y_pred)\n",
    "summary(act_vs_pred)\n",
    "\n",
    "\"\"\"\n",
    "From the below output we can see that all English texts in the test file were predicted correctly. Some of the Afrikaans\n",
    "texts were predicted as English. None of the Dutch texts were predicted correctly. Roughly 70% of them were predicted as \n",
    "Afrikaans, with the rest being predicted as English. This means that we'll have to address the class imbalance problem\n",
    "we have in this dataset.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Addressing the class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.4459725 ,  0.44768856, 13.38181818])"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import class_weight\n",
    "class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                 np.unique(y_train),\n",
    "                                                 y_train)\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.33791749,  1.34306569, 40.14545455])"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = y_train.unique()\n",
    "training_totals_by_language = y_train.value_counts()\n",
    "total = sum(training_totals_by_language)\n",
    "class_weights_2 = np.array([1/(training_totals_by_language[1]/total), 1/(training_totals_by_language[0]/total), 1/(training_totals_by_language[2]/total)])\n",
    "class_weights_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.]])"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_11 (Embedding)     (None, 200, 300)          1333200   \n",
      "_________________________________________________________________\n",
      "lstm_21 (LSTM)               (None, 200, 256)          570368    \n",
      "_________________________________________________________________\n",
      "lstm_22 (LSTM)               (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 3)                 771       \n",
      "=================================================================\n",
      "Total params: 2,429,651\n",
      "Trainable params: 2,429,651\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "2208/2208 [==============================] - 46s 21ms/step - loss: 0.4462 - accuracy: 0.8048\n",
      "Epoch 2/5\n",
      "2208/2208 [==============================] - 43s 20ms/step - loss: 0.3222 - accuracy: 0.8622\n",
      "Epoch 3/5\n",
      "2208/2208 [==============================] - 45s 20ms/step - loss: 0.2239 - accuracy: 0.9244\n",
      "Epoch 4/5\n",
      "2208/2208 [==============================] - 43s 19ms/step - loss: 0.1382 - accuracy: 0.9604\n",
      "Epoch 5/5\n",
      "2208/2208 [==============================] - 42s 19ms/step - loss: 0.0837 - accuracy: 0.9724\n",
      "Accuracy: 98.13%\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "model2 = Sequential()\n",
    "\n",
    "model2.add(Embedding(len(vocab_to_int), embedding_vector_length, input_length=max_sentence_length))\n",
    "model2.add(LSTM(256, return_sequences=True, dropout=dropout, recurrent_dropout=dropout))\n",
    "model2.add(LSTM(256, dropout=dropout, recurrent_dropout=dropout))\n",
    "model2.add(Dense(len(languages), activation='softmax'))\n",
    "\n",
    "model2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print(model2.summary())\n",
    "\n",
    "# Train the model\n",
    "model2.fit(X_train_pad, y_train_encoded, epochs=5, batch_size=256, class_weight = class_weights_2)\n",
    "\n",
    "# Final evaluation of the initial model\n",
    "scores = model2.evaluate(X_test_pad, y_test_encoded, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions\n",
    "y_pred = []\n",
    "\n",
    "for text in X_test:\n",
    "    y_pred.append(predict_sentence(model2, text)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>Afrikaans</th>\n",
       "      <th>English</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Afrikaans</td>\n",
       "      <td>127</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>English</td>\n",
       "      <td>0</td>\n",
       "      <td>411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Nederlands</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted   Afrikaans  English\n",
       "Actual                        \n",
       "Afrikaans         127        3\n",
       "English             0      411\n",
       "Nederlands          8        4"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act_vs_pred = create_df(y_pred)\n",
    "summary_df = summary(act_vs_pred)\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Afrikaans accuracy = 97.6923076923077%\n",
      "English accuracy = 100.0%\n",
      "Dutch accuracy = 0%\n"
     ]
    }
   ],
   "source": [
    "Afr_acc = summary_df['Afrikaans'][0]/(summary_df['Afrikaans'][0]+summary_df['English'][0])\n",
    "Eng_acc = summary_df['English'][1]/(summary_df['Afrikaans'][1]+summary_df['English'][1])\n",
    "\n",
    "print(f\"Afrikaans accuracy = {Afr_acc*100}%\")\n",
    "print(f\"English accuracy = {Eng_acc*100}%\")\n",
    "print(f\"Dutch accuracy = {0}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# weight_constraint = [1, 2, 3, 4, 5]\n",
    "# dropout_rate = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "# param_grid = dict(dropout_rate=dropout_rate, weight_constraint=weight_constraint)\n",
    "\n",
    "# grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "# grid_result = grid.fit(X_test_pad, y_train_encoded)\n",
    "# # summarize results\n",
    "# print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "# means = grid_result.cv_results_['mean_test_score']\n",
    "# stds = grid_result.cv_results_['std_test_score']\n",
    "# params = grid_result.cv_results_['params']\n",
    "# for mean, stdev, param in zip(means, stds, params):\n",
    "#     print(\"%f (%f) with: %r\" % (mean, stdev, param))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the final (tuned) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Final evaluation of the initial model\n",
    "# scores = model.evaluate(X_test_pad, y_test_encoded, verbose=0)\n",
    "# print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Afrikaans'"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentence(\"ontwyk die uitvraer want hy is ook ‘n verklapper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'English'"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentence(\"It is common to pre-select an optimization algorithm to train your network and tune its parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Afrikaans'"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentence(\"Goedemorgen. Ik hoop dat mijn model goed presteert\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the vocabulary to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# save to file\n",
    "with open('vocab.txt', 'w') as file:\n",
    "    json.dump(vocab_to_int, file)\n",
    "\n",
    "# check - read dictionary from file\n",
    "with open('vocab.txt', 'r') as file:\n",
    "    orig_vocab_to_int = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pk\n",
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# serialize model to JSON\n",
    "model_json = model2.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model2.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n",
      "Accuracy: 98.13%\n"
     ]
    }
   ],
   "source": [
    "# load json and create model\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    " \n",
    "# evaluate loaded model on test data\n",
    "loaded_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "scores = loaded_model.evaluate(X_test_pad, y_test_encoded, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the original test dataframe as CSV - this will be used at a later stage\n",
    "# to test whether the loaded model returns the correct result \n",
    "# (expect to see an accuracy of 98.13%)\n",
    "lang_data_test = pd.DataFrame(data = [X_test, y_test]).T\n",
    "\n",
    "lang_data_test.head()\n",
    "# saving lang_data_test to CSV\n",
    "lang_data_test.to_csv('lang_data_test.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.]])"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1402                              keep a stiff upper lip\n",
      "2616    ontwyk die uitvraer want hy is ook ‘n verklapper\n",
      "2292            there is more than one way to kill a cat\n",
      "2108            fools rush in where angels fear to tread\n",
      "464                                       meat and drink\n",
      "Name: text, dtype: object\n",
      "1402      English\n",
      "2616    Afrikaans\n",
      "2292      English\n",
      "2108      English\n",
      "464       English\n",
      "Name: language, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(X_test[:5])\n",
    "print(y_test[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
